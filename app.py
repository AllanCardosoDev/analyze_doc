"""
Analyse Doc - Aplica√ß√£o principal
Sistema avan√ßado de an√°lise de documentos com IA
"""
import tempfile
import os
import logging
import streamlit as st
from langchain.memory import ConversationBufferMemory
from langchain_groq import ChatGroq
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from typing import Optional, Generator

# Imports locais
from loaders import (
    carrega_site,
    carrega_youtube,
    carrega_pdf,
    carrega_csv,
    carrega_txt,
    carrega_docx
)
from document_memory import DocumentMemoryManager
from config import AppConfig, ModelConfig, FileTypes, CUSTOM_CSS
from utils import (
    validate_api_key,
    format_document_info,
    estimate_tokens,
    estimate_cost,
    safe_session_state_get,
    safe_session_state_set,
    clear_session_state_prefix,
    setup_logging
)

# Configurar logging
setup_logging()
logger = logging.getLogger(__name__)

# Configura√ß√µes da interface
st.set_page_config(
    page_title=AppConfig.APP_TITLE,
    page_icon=AppConfig.APP_ICON,
    layout=AppConfig.LAYOUT
)

# Aplicar estilos customizados
st.markdown(CUSTOM_CSS, unsafe_allow_html=True)


def initialize_session_state():
    """Inicializa o estado da sess√£o com valores padr√£o."""
    defaults = {
        "memoria": ConversationBufferMemory(),
        "chain": None,
        "doc_memory_manager": None,
        "documento_completo": None,
        "tipo_arquivo": None,
        "tamanho_documento": 0,
        "num_paginas": 0,
        "usando_documento_grande": False,
        "doc_chunks": [],
        "doc_hash": "",
        "chunk_size": AppConfig.DEFAULT_CHUNK_SIZE,
        "chunk_overlap": AppConfig.DEFAULT_CHUNK_OVERLAP,
        "k_chunks": AppConfig.DEFAULT_K_CHUNKS,
        "total_queries": 0,
        "use_embeddings": False,
        "messages_count": 0
    }
    
    for key, default_value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = default_value


def carrega_arquivos(tipo_arquivo: str, arquivo) -> tuple:
    """
    Fun√ß√£o para carregar arquivos com tratamento de erros.
    
    Args:
        tipo_arquivo: Tipo do arquivo a ser carregado
        arquivo: Arquivo ou URL
        
    Returns:
        tuple: (conte√∫do, mensagem)
    """
    if not arquivo:
        logger.warning("Nenhum arquivo ou URL fornecido.")
        return "", "‚ùå Nenhum arquivo ou URL fornecido."
    
    try:
        if tipo_arquivo == "Site":
            return carrega_site(arquivo)
        
        elif tipo_arquivo == "Youtube":
            return carrega_youtube(arquivo)
        
        # Para outros tipos de arquivo, criar arquivo tempor√°rio
        with tempfile.NamedTemporaryFile(delete=False, suffix=f".{tipo_arquivo.lower()}") as temp:
            temp.write(arquivo.read())
            temp_path = temp.name
        
        try:
            if tipo_arquivo == "Pdf":
                resultado = carrega_pdf(temp_path)
            elif tipo_arquivo == "Docx":
                resultado = carrega_docx(temp_path)
            elif tipo_arquivo == "Csv":
                resultado = carrega_csv(temp_path)
            elif tipo_arquivo == "Txt":
                resultado = carrega_txt(temp_path)
            else:
                resultado = ("", f"‚ùå Tipo de arquivo n√£o suportado: {tipo_arquivo}")
            
            return resultado
            
        finally:
            # Sempre tentar remover o arquivo tempor√°rio
            try:
                os.unlink(temp_path)
            except Exception as cleanup_err:
                logger.error(f"Erro ao limpar arquivo tempor√°rio: {cleanup_err}")
                
    except Exception as e:
        logger.error(f"Erro ao carregar arquivo: {e}")
        return "", f"‚ùå Erro ao carregar arquivo: {str(e)}"


def test_api_key(provider: str, api_key: str, model: str) -> tuple:
    """
    Testa se a API key √© v√°lida fazendo uma chamada simples.
    
    Args:
        provider: Provedor (Groq ou OpenAI)
        api_key: Chave API
        model: Nome do modelo
        
    Returns:
        tuple: (sucesso: bool, mensagem: str)
    """
    try:
        if provider == "Groq":
            chat = ChatGroq(model=model, api_key=api_key, temperature=0)
        else:
            chat = ChatOpenAI(model=model, api_key=api_key, temperature=0)
        
        # Fazer uma chamada simples de teste
        response = chat.invoke("Hi")
        
        if response:
            return True, "‚úÖ API key v√°lida!"
        return False, "‚ùå Resposta inv√°lida da API"
        
    except Exception as e:
        error_msg = str(e).lower()
        if "api key" in error_msg or "unauthorized" in error_msg or "invalid" in error_msg:
            return False, "‚ùå API key inv√°lida ou sem permiss√£o"
        elif "rate limit" in error_msg:
            return False, "‚ö†Ô∏è Limite de taxa excedido. Tente novamente mais tarde."
        else:
            return False, f"‚ùå Erro ao testar API: {str(e)[:100]}"


def carrega_modelo(provedor: str, modelo: str, api_key: str, tipo_arquivo: str, arquivo):
    """Carrega o modelo de IA e prepara o sistema para responder com base no documento."""
    
    # Validar API key
    if not api_key:
        api_key = safe_session_state_get(f'api_key_{provedor}', '')
    
    if not api_key:
        st.error("‚ö†Ô∏è API Key n√£o fornecida. Adicione uma chave v√°lida para continuar.")
        return
    
    # Validar formato da API key
    is_valid, msg = validate_api_key(api_key, provedor)
    if not is_valid:
        st.error(f"‚ö†Ô∏è {msg}")
        return
    
    # Mostrar progresso
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    try:
        # Passo 1: Testar API key
        status_text.text("üîë Validando API key...")
        progress_bar.progress(10)
        
        # Comentar teste para acelerar (opcional)
        # test_success, test_msg = test_api_key(provedor, api_key, modelo)
        # if not test_success:
        #     st.error(test_msg)
        #     return
        
        # Passo 2: Carregar documento
        status_text.text("üìÑ Carregando documento...")
        progress_bar.progress(30)
        
        documento, load_msg = carrega_arquivos(tipo_arquivo, arquivo)
        
        if not documento or documento.startswith("‚ùå"):
            st.error(load_msg if load_msg else "Documento n√£o p√¥de ser carregado")
            progress_bar.empty()
            status_text.empty()
            return
        
        # Mostrar mensagem de carregamento
        if load_msg.startswith("‚úÖ"):
            st.success(load_msg)
        
        # Passo 3: Processar documento
        status_text.text("‚öôÔ∏è Processando documento...")
        progress_bar.progress(50)
        
        # Armazenar o documento completo na sess√£o
        safe_session_state_set('documento_completo', documento)
        safe_session_state_set('tamanho_documento', len(documento))
        safe_session_state_set('tipo_arquivo', tipo_arquivo)
        
        # Inicializar o gerenciador de mem√≥ria de documentos
        use_embeddings = safe_session_state_get('use_embeddings', False)
        
        if 'doc_memory_manager' not in st.session_state or st.session_state['doc_memory_manager'] is None:
            st.session_state['doc_memory_manager'] = DocumentMemoryManager(use_embeddings=use_embeddings)
        
        memory_manager = st.session_state['doc_memory_manager']
        
        # Obter configura√ß√µes de chunking
        chunk_size = safe_session_state_get('chunk_size', AppConfig.DEFAULT_CHUNK_SIZE)
        chunk_overlap = safe_session_state_get('chunk_overlap', AppConfig.DEFAULT_CHUNK_OVERLAP)
        
        # Processar com o gerenciador de mem√≥ria
        processamento = memory_manager.process_document(
            documento, 
            tipo_arquivo,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        
        # Passo 4: Criar prompt e chain
        status_text.text("ü§ñ Configurando modelo de IA...")
        progress_bar.progress(70)
        
        # Para documentos grandes (mais de threshold caracteres)
        if len(documento) > AppConfig.SMALL_DOCUMENT_THRESHOLD:
            safe_session_state_set('usando_documento_grande', True)
            documento_preview = memory_manager.get_document_preview(max_chars=1500)
            
            system_message = f"""Voc√™ √© um assistente especializado em an√°lise de documentos.
            
Voc√™ possui acesso a informa√ß√µes de um documento do tipo {tipo_arquivo}.

Caracter√≠sticas do documento:
- Tamanho: {len(documento)} caracteres
- P√°ginas: aproximadamente {processamento['num_paginas']} p√°ginas
- Processado em {processamento['total_chunks']} chunks

Preview do documento:
####
{documento_preview}
####

Este √© apenas um trecho inicial. Voc√™ tem acesso ao documento completo atrav√©s de um sistema 
de recupera√ß√£o que fornecer√° as informa√ß√µes mais relevantes para cada pergunta do usu√°rio.

Instru√ß√µes:
1. Utilize as informa√ß√µes do documento para responder √†s perguntas do usu√°rio
2. Seja direto, preciso e √∫til nas suas respostas
3. Se n√£o encontrar informa√ß√£o espec√≠fica no contexto fornecido, indique isso claramente
4. Voc√™ pode fazer refer√™ncia a perguntas anteriores quando relevante
5. Cite trechos espec√≠ficos do documento quando apropriado
"""
        else:
            safe_session_state_set('usando_documento_grande', False)
            
            system_message = f"""Voc√™ √© um assistente especializado em an√°lise de documentos.

Voc√™ possui acesso completo a um documento do tipo {tipo_arquivo}:

####
{documento}
####

Instru√ß√µes:
1. Utilize as informa√ß√µes do documento para responder √†s perguntas do usu√°rio
2. Seja direto, preciso e √∫til nas suas respostas
3. Cite trechos espec√≠ficos do documento quando apropriado
4. Voc√™ pode fazer refer√™ncia a perguntas anteriores quando relevante
"""
        
        template = ChatPromptTemplate.from_messages([
            ('system', system_message),
            ('placeholder', '{chat_history}'),
            ('user', '{input}')
        ])
        
        # Passo 5: Inicializar modelo
        status_text.text("üöÄ Inicializando modelo...")
        progress_bar.progress(90)
        
        config_modelo = ModelConfig.PROVIDERS[provedor]
        temperatura = config_modelo.get('temperatura_padrao', 0.7)
        
        if provedor == 'Groq':
            chat = ChatGroq(
                model=modelo,
                api_key=api_key,
                temperature=temperatura
            )
        else:
            chat = ChatOpenAI(
                model=modelo,
                api_key=api_key,
                temperature=temperatura
            )
        
        chain = template | chat
        
        # Guardar na sess√£o
        safe_session_state_set('chain', chain)
        safe_session_state_set('current_provider', provedor)
        safe_session_state_set('current_model', modelo)
        
        # Finalizar
        progress_bar.progress(100)
        status_text.empty()
        progress_bar.empty()
        
        # Mostrar informa√ß√µes do documento
        info = memory_manager.get_document_info()
        st.sidebar.success(f"‚úÖ Documento {tipo_arquivo} carregado com sucesso!")
        
        # Estat√≠sticas
        st.sidebar.markdown(format_document_info(info), unsafe_allow_html=True)
        
        # Estimar custo
        if processamento.get('estimated_tokens'):
            cost_info = estimate_cost(
                processamento['estimated_tokens'],
                provedor,
                modelo
            )
            st.sidebar.info(
                f"üí∞ Custo estimado por consulta: ${cost_info['total_estimated']:.4f}"
            )
        
    except Exception as e:
        logger.error(f"Erro ao carregar modelo: {e}", exc_info=True)
        st.error(f"‚ùå Erro ao processar documento: {str(e)}")
        progress_bar.empty()
        status_text.empty()


def processar_pergunta_com_documento(
    input_usuario: str, 
    chain, 
    memoria: ConversationBufferMemory
) -> Generator[str, None, None]:
    """
    Processa perguntas usando chunks relevantes do documento.
    Utiliza a mem√≥ria de conversa√ß√£o para manter contexto.
    """
    try:
        # Obter o gerenciador de mem√≥ria de documentos
        memory_manager = safe_session_state_get('doc_memory_manager')
        
        if not memory_manager:
            yield "‚ùå Erro: Sistema n√£o conseguiu acessar o documento. Por favor, tente recarregar."
            return
        
        # Obter o n√∫mero de chunks configurado pelo usu√°rio
        k_chunks = safe_session_state_get('k_chunks', AppConfig.DEFAULT_K_CHUNKS)
        
        # Recuperar chunks relevantes para a pergunta
        chunks_relevantes = memory_manager.retrieve_relevant_chunks(input_usuario, k=k_chunks)
        
        # Combinar o conte√∫do dos chunks relevantes
        contexto_relevante = "\n\n".join([chunk.page_content for chunk in chunks_relevantes])
        
        # Criar um prompt que inclui os chunks relevantes
        prompt_adicional = f"""
Para responder √† pergunta atual, use estas informa√ß√µes relevantes do documento:

{contexto_relevante}

Pergunta: {input_usuario}
"""
        
        # Usar o chain para gerar a resposta
        resposta = ""
        for chunk in chain.stream({
            "input": prompt_adicional,
            "chat_history": memoria.buffer_as_messages
        }):
            if hasattr(chunk, 'content'):
                resposta += chunk.content
            else:
                resposta += str(chunk)
            yield resposta
        
        # Incrementar contador de consultas
        total_queries = safe_session_state_get('total_queries', 0)
        safe_session_state_set('total_queries', total_queries + 1)
        
    except Exception as e:
        logger.error(f"Erro ao processar pergunta: {e}", exc_info=True)
        yield f"‚ùå Erro ao processar sua pergunta: {str(e)}"


def pagina_chat():
    """Interface principal do chat."""
    st.markdown('<h1 class="main-header">üìë Analyse Doc</h1>', unsafe_allow_html=True)
    
    chain = safe_session_state_get('chain')
    
    if chain is None:
        st.info("üëà Carregue um documento na barra lateral para come√ßar a conversar.")
        
        with st.expander("‚ÑπÔ∏è Como usar o Analyse Doc"):
            st.markdown("""
            ### Guia R√°pido de Uso
            
            **1. Selecione o tipo de documento** na barra lateral
            - Site, YouTube, PDF, Word, CSV ou TXT
            
            **2. Carregue o documento**
            - Para sites e YouTube: cole a URL
            - Para arquivos: fa√ßa o upload
            
            **3. Escolha o modelo de IA**
            - Groq: modelos r√°pidos e gratuitos
            - OpenAI: modelos mais avan√ßados (GPT-4, etc)
            
            **4. Adicione sua API Key**
            - Groq: obtenha em https://console.groq.com
            - OpenAI: obtenha em https://platform.openai.com
            
            **5. Clique em "Inicializar"**
            - O sistema processar√° seu documento
            
            **6. Fa√ßa perguntas sobre o documento**
            - Digite suas perguntas no campo abaixo
            - O sistema buscar√° informa√ß√µes relevantes para responder
            
            ### Dicas
            - Para documentos grandes, o sistema usa chunks inteligentes
            - Voc√™ pode ajustar o tamanho dos chunks nas configura√ß√µes
            - O hist√≥rico do chat √© mantido durante a sess√£o
            - Use "Limpar Chat" para iniciar uma nova conversa
            """)
        
        # Mostrar estat√≠sticas de uso se dispon√≠vel
        if safe_session_state_get('total_queries', 0) > 0:
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("üìä Consultas", safe_session_state_get('total_queries', 0))
            with col2:
                st.metric("üí¨ Mensagens", safe_session_state_get('messages_count', 0))
            with col3:
                if safe_session_state_get('tipo_arquivo'):
                    st.metric("üìÑ Tipo", safe_session_state_get('tipo_arquivo'))
        
        st.stop()
    
    # Recupera a mem√≥ria da sess√£o
    memoria = safe_session_state_get('memoria', ConversationBufferMemory())
    
    # Container para o chat
    chat_container = st.container()
    
    with chat_container:
        # Exibir o hist√≥rico de mensagens
        messages = memoria.buffer_as_messages
        
        if len(messages) == 0:
            st.info("üí° Fa√ßa sua primeira pergunta sobre o documento!")
        
        for mensagem in messages:
            if mensagem.type == 'ai':
                st.markdown(
                    f'<div class="chat-message-ai">ü§ñ {mensagem.content}</div>', 
                    unsafe_allow_html=True
                )
            else:
                st.markdown(
                    f'<div class="chat-message-human">üë§ {mensagem.content}</div>', 
                    unsafe_allow_html=True
                )
    
    # Campo de entrada do usu√°rio
    input_usuario = st.chat_input("Fa√ßa perguntas sobre o documento carregado...")
    
    if input_usuario:
        # Incrementar contador de mensagens
        messages_count = safe_session_state_get('messages_count', 0)
        safe_session_state_set('messages_count', messages_count + 1)
        
        # Exibir a mensagem do usu√°rio
        with chat_container:
            st.markdown(
                f'<div class="chat-message-human">üë§ {input_usuario}</div>', 
                unsafe_allow_html=True
            )
        
        try:
            with st.spinner("üîç Analisando..."):
                # Configura√ß√£o para streaming de resposta
                with chat_container:
                    resposta_container = st.empty()
                    
                    # Processar pergunta com documento
                    for resposta_parcial in processar_pergunta_com_documento(
                        input_usuario, 
                        chain, 
                        memoria
                    ):
                        resposta_container.markdown(
                            f'<div class="chat-message-ai">ü§ñ {resposta_parcial}</div>',
                            unsafe_allow_html=True
                        )
                    
                    resposta_completa = resposta_parcial
            
            # Adicionar √† mem√≥ria
            memoria.chat_memory.add_user_message(input_usuario)
            memoria.chat_memory.add_ai_message(resposta_completa)
            safe_session_state_set('memoria', memoria)
            
        except Exception as e:
            logger.error(f"Erro ao processar resposta: {e}", exc_info=True)
            with chat_container:
                st.error(f"‚ùå Erro ao processar resposta: {str(e)}")


def sidebar():
    """Cria a barra lateral para upload de arquivos e sele√ß√£o de modelos."""
    st.sidebar.header("üõ†Ô∏è Configura√ß√µes")
    
    tabs = st.sidebar.tabs(['üìÅ Upload', 'ü§ñ Modelo', '‚öôÔ∏è Avan√ßado'])
    
    # TAB 1: Upload de Arquivos
    with tabs[0]:
        st.subheader("Carregar Documento")
        
        tipo_arquivo = st.selectbox(
            'Tipo de documento',
            FileTypes.SUPPORTED_TYPES,
            help="Selecione o tipo de documento que deseja analisar"
        )
        
        # Interface de acordo com o tipo de arquivo
        arquivo = None
        
        if tipo_arquivo == 'Site':
            arquivo = st.text_input(
                'URL do site',
                placeholder="https://exemplo.com",
                help="Cole a URL completa do site"
            )
        elif tipo_arquivo == 'Youtube':
            arquivo = st.text_input(
                'URL do v√≠deo',
                placeholder="https://www.youtube.com/watch?v=...",
                help="Cole a URL do v√≠deo do YouTube"
            )
        elif tipo_arquivo == 'Pdf':
            arquivo = st.file_uploader(
                'Upload PDF',
                type=['pdf'],
                help=f"Tamanho m√°ximo: {AppConfig.MAX_FILE_SIZE_MB} MB"
            )
        elif tipo_arquivo == 'Docx':
            arquivo = st.file_uploader(
                'Upload Word',
                type=['docx'],
                help=f"Tamanho m√°ximo: {AppConfig.MAX_FILE_SIZE_MB} MB"
            )
        elif tipo_arquivo == 'Csv':
            arquivo = st.file_uploader(
                'Upload CSV',
                type=['csv'],
                help=f"Tamanho m√°ximo: {AppConfig.MAX_FILE_SIZE_MB} MB"
            )
        elif tipo_arquivo == 'Txt':
            arquivo = st.file_uploader(
                'Upload TXT',
                type=['txt'],
                help=f"Tamanho m√°ximo: {AppConfig.MAX_FILE_SIZE_MB} MB"
            )
    
    # TAB 2: Sele√ß√£o de Modelos
    with tabs[1]:
        st.subheader("Configurar IA")
        
        provedor = st.selectbox(
            'Provedor',
            ModelConfig.PROVIDERS.keys(),
            help="Escolha o provedor de IA"
        )
        
        modelo = st.selectbox(
            'Modelo',
            ModelConfig.PROVIDERS[provedor]['modelos'],
            help="Escolha o modelo espec√≠fico"
        )
        
        # Campo de API key com persist√™ncia
        api_key_default = safe_session_state_get(f'api_key_{provedor}', '')
        api_key = st.text_input(
            f'API Key ({provedor})',
            type="password",
            value=api_key_default,
            help=f"Sua chave API do {provedor}"
        )
        
        if api_key:
            safe_session_state_set(f'api_key_{provedor}', api_key)
            
            # Validar formato
            is_valid, msg = validate_api_key(api_key, provedor)
            if is_valid:
                st.success("‚úÖ Formato v√°lido")
            else:
                st.warning(f"‚ö†Ô∏è {msg}")
    
    # TAB 3: Configura√ß√µes Avan√ßadas
    with tabs[2]:
        st.subheader("Processamento")
        
        # Mostrar informa√ß√µes do documento atual se dispon√≠vel
        if 'doc_memory_manager' in st.session_state and st.session_state['doc_memory_manager'] is not None:
            memory_manager = st.session_state['doc_memory_manager']
            
            try:
                info = memory_manager.get_document_info()
                
                st.markdown("**üìä Documento Atual**")
                st.text(f"Tipo: {info.get('tipo', 'N/A')}")
                st.text(f"Tamanho: {info.get('tamanho', 0):,} caracteres")
                st.text(f"P√°ginas: ~{info.get('num_paginas', 0)}")
                st.text(f"Chunks: {info.get('num_chunks', 0)}")
                st.text(f"Tokens: ~{info.get('estimated_tokens', 0):,}")
                
                st.markdown("---")
            except Exception as e:
                logger.error(f"Erro ao obter info do documento: {e}")
        
        # Configura√ß√µes de chunking
        st.caption("**Tamanho dos Chunks**")
        chunk_size = st.slider(
            "Caracteres por chunk",
            min_value=AppConfig.MIN_CHUNK_SIZE,
            max_value=AppConfig.MAX_CHUNK_SIZE,
            value=safe_session_state_get('chunk_size', AppConfig.DEFAULT_CHUNK_SIZE),
            step=500,
            help="Chunks menores = menos tokens, mas podem perder contexto"
        )
        safe_session_state_set('chunk_size', chunk_size)
        
        chunk_overlap = st.slider(
            "Sobreposi√ß√£o",
            min_value=0,
            max_value=500,
            value=safe_session_state_get('chunk_overlap', AppConfig.DEFAULT_CHUNK_OVERLAP),
            step=50,
            help="Overlap entre chunks para manter continuidade"
        )
        safe_session_state_set('chunk_overlap', chunk_overlap)
        
        st.caption("**Recupera√ß√£o de Contexto**")
        k_chunks = st.slider(
            "Chunks por consulta",
            min_value=AppConfig.MIN_K_CHUNKS,
            max_value=AppConfig.MAX_K_CHUNKS,
            value=safe_session_state_get('k_chunks', AppConfig.DEFAULT_K_CHUNKS),
            step=1,
            help="Mais chunks = mais contexto, mas mais tokens"
        )
        safe_session_state_set('k_chunks', k_chunks)
        
        # Op√ß√£o de usar embeddings (experimental)
        st.caption("**Recursos Experimentais**")
        use_embeddings = st.checkbox(
            "Usar busca vetorial",
            value=safe_session_state_get('use_embeddings', False),
            help="‚ö†Ô∏è Requer mais mem√≥ria, mas melhora a precis√£o"
        )
        safe_session_state_set('use_embeddings', use_embeddings)
    
    # Bot√µes de a√ß√£o
    st.sidebar.markdown("---")
    col1, col2 = st.sidebar.columns(2)
    
    with col1:
        if st.button('üöÄ Inicializar', use_container_width=True, type="primary"):
            if not arquivo:
                st.sidebar.error("‚ùå Selecione um documento primeiro!")
            elif not api_key:
                st.sidebar.error("‚ùå Adicione sua API key!")
            else:
                with st.spinner("Processando..."):
                    carrega_modelo(provedor, modelo, api_key, tipo_arquivo, arquivo)
    
    with col2:
        if st.button('üóëÔ∏è Limpar', use_container_width=True):
            # Limpar apenas o hist√≥rico do chat
            st.session_state['memoria'] = ConversationBufferMemory()
            st.session_state['messages_count'] = 0
            st.sidebar.success("‚úÖ Chat limpo!")
            st.rerun()
    
    # Bot√£o para reiniciar completamente
    if st.sidebar.button('üîÑ Novo Documento', use_container_width=True):
        # Limpar tudo exceto API keys
        keys_to_keep = [k for k in st.session_state.keys() if k.startswith('api_key_')]
        keys_to_remove = [k for k in st.session_state.keys() if k not in keys_to_keep]
        
        for key in keys_to_remove:
            del st.session_state[key]
        
        initialize_session_state()
        st.sidebar.success("‚úÖ Sistema reiniciado!")
        st.rerun()
    
    # Informa√ß√µes adicionais na sidebar
    st.sidebar.markdown("---")
    st.sidebar.caption("**üìà Estat√≠sticas da Sess√£o**")
    
    if safe_session_state_get('chain') is not None:
        col1, col2 = st.sidebar.columns(2)
        with col1:
            st.metric("Consultas", safe_session_state_get('total_queries', 0))
        with col2:
            st.metric("Mensagens", safe_session_state_get('messages_count', 0))
    
    # Footer
    st.sidebar.markdown("---")
    st.sidebar.caption("**‚ÑπÔ∏è Sobre**")
    st.sidebar.info(
        "**Analyse Doc**\n\n"
        "An√°lise inteligente de documentos com IA\n\n"
        "Desenvolvido por Allan Cardoso"
    )


def main():
    """Fun√ß√£o principal."""
    # Inicializar estado da sess√£o
    initialize_session_state()
    
    # Renderizar sidebar
    sidebar()
    
    # Renderizar p√°gina de chat
    pagina_chat()


if __name__ == '__main__':
    main()
